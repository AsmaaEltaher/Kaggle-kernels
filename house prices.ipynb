{
  "cells": [
    {
      "metadata": {
        "_uuid": "fabc54b78db4b9f9828e6675d774f708f59a1725",
        "_cell_guid": "f5f943dc-8a08-41f0-8aef-1ae6bd44b870"
      },
      "cell_type": "markdown",
      "source": "# Starting Your ML Project\n**Run the equivalent commands (to read the data and print the summary) in the code cell below. **"
    },
    {
      "metadata": {
        "_uuid": "1c728098629e1301643443b1341556a15c089b2b",
        "_cell_guid": "86b26423-563a-4fa1-a595-89e25ff93089",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nmain_file_path = '../input/train.csv'\ndata = pd.read_csv(main_file_path)\nprint(data.describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c082c316652a43cccbe9245cd8a9d438c6985af",
        "_cell_guid": "6cd5e250-a2f8-4378-99fd-a97813b5a642"
      },
      "cell_type": "markdown",
      "source": "# Selecting and Filtering in Pandas\n* Print a list of the columns\n* From the list of columns, find a name of the column with the sales prices of the homes. Use the dot notation to extract this to a variable (as you saw above to create melbourne_price_data.)\n* Use the head command to print out the top few lines of the variable you just created.\n* Pick any two variables and store them to a new DataFrame (as you saw above to create two_columns_of_data.)\n* Use the describe command with the DataFrame you just created to see summaries of those variables."
    },
    {
      "metadata": {
        "_uuid": "77504004acd600d1bc6664500ed8583fa8fa914f",
        "_cell_guid": "cee50123-aec4-4c45-9fb3-86f2e964ab5c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(data.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d5de7812e6d70256c2e12db477bb2f758a7407fb",
        "_cell_guid": "ff794619-e642-4f6a-bb45-9e713bad3143",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data_price=data.SalePrice\nprint(data_price.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "5b9b85d570f8f3207800e46ff2b922f60dbbc8c7",
        "_cell_guid": "871a06c3-481d-4bbf-acb8-ce0399aabea0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "columns_of_interest=['LotArea','YearBuilt']\ntwo_columns_of_data=data[columns_of_interest]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c700dd5deaf56c220be5d28730c0746c7b30bebb",
        "_cell_guid": "04ced8f3-9337-4f33-bbe5-107a9fef04ba",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "two_columns_of_data.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec9b3ffbfd6e2e5c3fbc89436124d87c96643f87",
        "_cell_guid": "cce4fe22-ba73-4fb9-a5ec-3be0a3cac129"
      },
      "cell_type": "markdown",
      "source": "# Your First Scikit-Learn Model\n* Select the target variable you want to predict. You can go back to the list of columns from your earlier commands to recall what it's called (hint: you've already worked with this variable). Save this to a new variable called y.\n* Create a list of the names of the predictors we will use in the initial model. Use just the following columns in the list (you can copy and paste the whole list to save some typing, though you'll still need to add quotes):\n    * LotArea\n    * YearBuilt\n    * 1stFlrSF\n    * 2ndFlrSF\n    * FullBath\n    * BedroomAbvGr\n    * TotRmsAbvGrd\n* Using the list of variable names you just created, select a new DataFrame of the predictors data. Save this with the variable name X.\n\n* Create a DecisionTreeRegressorModel and save it to a variable (with a name like my_model or iowa_model). Ensure you've done the relevant import so you can run this command.\n* Fit the model you have created using the data in X and the target data you saved above.\n* Make a few predictions with the model's predict command and print out the predictions."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "563559d1240f71f80506838e8f7d69bb2cb1d5e5",
        "_cell_guid": "2cd1c636-5ef6-4014-a8b8-2fa5f9121eff",
        "trusted": false
      },
      "cell_type": "code",
      "source": "Y=data.SalePrice",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "721f6bcc730d76ffb6b70594e9797790237f7e93",
        "_cell_guid": "9abf58b6-c71e-4d64-8762-5db2b3a91f85",
        "trusted": false
      },
      "cell_type": "code",
      "source": "data_predictors=['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath',\n                'BedroomAbvGr','TotRmsAbvGrd']\nX=data[data_predictors]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "413a67e5dc48efe0619201f0a59bc89ca9102468",
        "_cell_guid": "2dc4b368-1949-48d0-8505-fe2eb8cc2035",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeRegressor\n\niowa_model=DecisionTreeRegressor()\n\niowa_model.fit(X,Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fb1d2aecb71dafbd095da03e4a760a3b26587b75",
        "_cell_guid": "fbc25d85-5468-41ff-89f6-8338a96347b6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"the predictions are:\")\nprint(iowa_model.predict(X.head()))\nprint(\"the real prices is:\")\nprint(Y.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6394d40602942958c250918cac37981e5c7e9b4c",
        "_cell_guid": "5c38f3a4-0cfc-40fa-bb9d-ed158fcc0058"
      },
      "cell_type": "markdown",
      "source": "# Model Validation\n* Use the train_test_split command to split up your data.\n* Fit the model with the training data\n* Make predictions with the validation predictors\n* Calculate the mean absolute error between your predictions and the actual target values for the validation data."
    },
    {
      "metadata": {
        "_uuid": "92c3964809b57647585a6150f11eedc4a3d97c3b",
        "_cell_guid": "57bff10d-dcb9-4269-81ab-6bbe4b8aa531",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n#split data\ntrain_X, val_X, train_y, val_y=train_test_split(X,Y,random_state=0)\n#define model\niowa_model=DecisionTreeRegressor()\n#fit model\niowa_model.fit(train_X,train_y)\n\n#do prediction\nval_predictions= iowa_model.predict(val_X)\nprint(mean_absolute_error(val_y,val_predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c434d250fa6cbd2337f978074f44648f81c3808",
        "_cell_guid": "83c072eb-c4a7-4ea0-936b-6269c0fc33ea"
      },
      "cell_type": "markdown",
      "source": "# Underfitting, Overfitting and Model Optimization\n* Use a for loop that tries different values of max_leaf_nodes and calls the get_mae function on each to find the ideal number of leaves for your Iowa data."
    },
    {
      "metadata": {
        "_uuid": "a4fcea8ee89bf6ecd2d6a0a6d191508c61dbf6e9",
        "_cell_guid": "8214f574-3eb7-449a-84ec-a9a5e0a4d592",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def get_mae(max_leaf_nodes,predictors_train,predictors_val,targ_train,targ_val):\n    model=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0)\n    model.fit(predictors_train,targ_train)\n    preds_val= model.predict(predictors_val)\n    mae=mean_absolute_error(targ_val,preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [10,20,30,40,50,60,70,80,90,100,200,300,400,500,1000]:\n    my_mae=get_mae(max_leaf_nodes,train_X,val_X,train_y,val_y)\n    print(\"max leaf nodes: %d \\t\\t mean absolute error: %d\"%(max_leaf_nodes,my_mae))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50876e89d89d95b907237bc872e841459ec27a93",
        "_cell_guid": "e72d8fe9-2666-4aa7-835d-5b7ee5545f04"
      },
      "cell_type": "markdown",
      "source": "# Random Forests\n* Run the RandomForestRegressor on your data. You should see a big improvement over your best Decision Tree models."
    },
    {
      "metadata": {
        "_uuid": "788e0d33e66d232a844dd845fa86af5365f78e28",
        "_cell_guid": "1d3ede3d-ffbb-4f93-ab91-d0a2e08d7f0f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\n\nforest_model=RandomForestRegressor()\nforest_model.fit(train_X,train_y)\niowa_preds=forest_model.predict(val_X)\n\nprint(mean_absolute_error(val_y,iowa_preds))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ed410e621d5f6ead1c0c105f549acb4dac996fb",
        "_cell_guid": "fae7c2b0-bf32-4210-a9d1-546b2520eb6f"
      },
      "cell_type": "markdown",
      "source": "## Submitting \"Random Forests Model\" From A Kernel\n"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "2f92848ea813efd39d457451822f82745768232c",
        "_cell_guid": "67db5486-eb90-474d-b6a7-34b63312c9e2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ntrain=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\n\ntrain_y=train.SalePrice\npredictors_cols=['LotArea','OverallQual','YearBuilt','TotRmsAbvGrd']\ntrain_x=train[predictors_cols]\ntest_x=test[predictors_cols]\n\nmy_model=RandomForestRegressor()\nmy_model.fit(train_x,train_y)\n\npredicted_prices=my_model.predict(test_x)\nprint(predicted_prices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "4bf526062c28dea6e3a84d79c3560ad53950e194",
        "_cell_guid": "1549cec3-b43f-4980-8b0e-2cf4469d4b03",
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_submission = pd.DataFrame({'ID':test.Id, 'SalePrice':predicted_prices})\n\nmy_submission.to_csv('submission1.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ff33a6cdd9c05bdc649ca07344e42be53248e9ef",
        "_cell_guid": "b8fd8ca4-63df-4e48-be21-e23a59a3e0ad"
      },
      "cell_type": "markdown",
      "source": "# Handling Missing Values\n* Find some columns with missing values in your dataset.\n* Use the Imputer class so you can impute missing values\n* Add columns with missing values to your predictors."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "aed8b85a2fc04bb9f16eb0e60d5ccaeb41e027f2",
        "_cell_guid": "559b985a-1f3c-4e55-b005-1d9c3c594e77",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\ndata = pd.read_csv('../input/train.csv')\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\niowa_target = data.SalePrice\niowa_predictors = data.drop(['SalePrice'], axis=1)\n\niowa_numeric_predictors = iowa_predictors.select_dtypes(exclude=['object'])\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(iowa_numeric_predictors, iowa_target,random_state=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "49c05b32af430446bd1049b001624542f00397c3",
        "_cell_guid": "3e899622-d201-4acf-bc6a-3ad2f7000177",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from  sklearn.preprocessing import Imputer\n\ncols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\n\nmy_imputer=Imputer()\nimputed_X_train=my_imputer.fit_transform(X_train)\nimputed_X_test=my_imputer.fit_transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9ec7b6d0266a03dde9a5d36846c24b184d0eee4",
        "_cell_guid": "be7ceae5-1f91-4562-bd72-dd687904d2bc",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nmy_model=RandomForestRegressor()\nmy_model.fit(imputed_X_train,y_train)\npreds= my_model.predict(imputed_X_test)\nprint(mean_absolute_error(y_test,preds))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96243a616348688894132c96df1589bf6678a696",
        "_cell_guid": "f4bda726-fb7f-4b37-97ee-1f9dbaec6c86"
      },
      "cell_type": "markdown",
      "source": "## Submitting  Random Forests Model after Handling Missing Values"
    },
    {
      "metadata": {
        "_uuid": "60fcee28a55c4eafa846512aaa77a37682c70a57",
        "_cell_guid": "f56882ee-b32a-4767-b2f2-b40d2a9e5094",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom  sklearn.preprocessing import Imputer\n\niowa_train=pd.read_csv('../input/train.csv')\niowa_test=pd.read_csv('../input/test.csv')\n\ntrain=iowa_train.select_dtypes(exclude=['object'])\ntest=iowa_test.select_dtypes(exclude=['object'])\n\ny_train=train.SalePrice\nX_train= train.drop(['SalePrice'], axis=1)\n\nmy_imputer=Imputer()\nimputed_X_train=my_imputer.fit_transform(X_train)\nimputed_X_test=my_imputer.fit_transform(test)\n\nmy_model=RandomForestRegressor()\nmy_model.fit(imputed_X_train,y_train)\n\npredicted_prices=my_model.predict(imputed_X_test)\nprint(predicted_prices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "dcb4f23d01279f2cadaa816df0cfd9d38f3a999c",
        "_cell_guid": "1392eeb1-5dfe-40bd-9d45-ac5a4d64c64c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_submission = pd.DataFrame({'ID':test.Id, 'SalePrice':predicted_prices})\n\nmy_submission.to_csv('submission2.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0207154c7fc0784ea0f0604943a4c706e822c0f4",
        "_cell_guid": "ca15dd22-357f-4bc3-a268-305a283ee80d"
      },
      "cell_type": "markdown",
      "source": "# Using Categorical Data with One Hot Encoding\n* Use one-hot encoding to allow categoricals in your course project. Then add some categorical columns to your X data. If you choose the right variables, your model will improve quite a bit."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "1389e9e05dd9389300c805f86f52aec85fadf50c",
        "_cell_guid": "d90baa5c-42e8-42ec-940c-58105155dd52",
        "trusted": false
      },
      "cell_type": "code",
      "source": "target = data.SalePrice\ncols_with_missing = [col for col in data.columns \n                                 if data[col].isnull().any()]                                  \npredictors = data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(predictors, \n                                                    target,\n                                                    random_state=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7eebb5a011c6fabae01e0bf349c70195682211da",
        "_cell_guid": "6df653e1-9e3a-46dd-b8f7-0ebdbb2c3592",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "encoded_X_train=pd.get_dummies(X_train)\nencoded_X_test=pd.get_dummies(X_test)\nfinal_X_train, final_X_test= encoded_X_train.align(encoded_X_test,join='inner',axis=1)\n\nmy_model=RandomForestRegressor()\nmy_model.fit(final_X_train,y_train)\npreds= my_model.predict(final_X_test)\nprint(mean_absolute_error(y_test,preds))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": false,
        "collapsed": true,
        "_uuid": "92c116c8a53cf69b4662a6752565877f46751784",
        "_cell_guid": "fb28a9ea-3334-4f22-9543-25e95ff7b682",
        "_kg_hide-input": false
      },
      "cell_type": "markdown",
      "source": "## Submitting  Random Forests Model after use One Hot Encoding"
    },
    {
      "metadata": {
        "_kg_hide-output": false,
        "collapsed": true,
        "_uuid": "f33fa676b59512c076969a2d8c864f9a89b38e8b",
        "_cell_guid": "dfe05520-ff00-4c25-8d10-16ad785047ff",
        "_kg_hide-input": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom  sklearn.preprocessing import Imputer\n\niowa_train=pd.read_csv('../input/train.csv')\niowa_test=pd.read_csv('../input/test.csv')\n\ny_train= iowa_train.SalePrice\ncols_with_missing_train = [col for col in iowa_train.columns \n                                 if iowa_train[col].isnull().any()]   \nX_train= iowa_train.drop(['SalePrice'] + cols_with_missing_train, axis=1)\ncols_with_missing_test = [col for col in iowa_test.columns \n                                 if iowa_test[col].isnull().any()]  \nX_test= iowa_test.drop(cols_with_missing_test, axis=1)\n\nencoded_X_train=pd.get_dummies(X_train)\nencoded_X_test=pd.get_dummies(X_test)\nfinal_X_train, final_X_test= encoded_X_train.align(encoded_X_test,join='inner',axis=1)\n\nmy_model=RandomForestRegressor()\nmy_model.fit(final_X_train,y_train)\n\npredicted_prices=my_model.predict(final_X_test)\nprint(predicted_prices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "67e672a62ec8f343353f6b77bba17af5d9b88356",
        "_cell_guid": "5286e636-84c9-4dd8-b5b0-1bab5313e523",
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_submission = pd.DataFrame({'ID':X_test.Id, 'SalePrice':predicted_prices})\n\nmy_submission.to_csv('submission3.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ad3319d13eca1a7755bd526e932bd203582ef923",
        "_cell_guid": "a9786738-8d5c-48ad-b24b-7848c0822abd"
      },
      "cell_type": "markdown",
      "source": "# Use XGBoost\n* Convert your model to use XGBoost.\n* Use early stopping to find a good value for n_estimators. Then re-estimate the model with all of your training data, and that value of n_estimators."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "05f2075e7984e4138d978c8e4ca41023ebd99fb1",
        "_cell_guid": "93181de3-703b-4db0-9d5a-95bfc011b633",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\n\ndata = pd.read_csv('../input/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny=data.SalePrice\nX=data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y= train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer=Imputer()\ntrain_X=my_imputer.fit_transform(train_X)\ntest_X=my_imputer.transform(test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b32026517c8e0e743036b31157d5d848dd859aae",
        "_cell_guid": "b0b2187a-31be-4b5d-b01d-92d7a1fb5747",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor\n\nmy_model=XGBRegressor()\nmy_model.fit(train_X,train_y, verbose=False)\n\npredictions= my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean absolute error: \"+str(mean_absolute_error(predictions,test_y)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e9c1278427261f990ea60876cb3c08f345fef294",
        "_cell_guid": "9387c978-40eb-4d81-b0e0-f18d86023039",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "my_model=XGBRegressor(n_estimators=1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5,\n            eval_set=[(test_X,test_y)], verbose=False)\n\npredictions= my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean absolute error: \"+str(mean_absolute_error(predictions,test_y)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a53a2e1496cb081db20745108cd4d772618355d4",
        "_cell_guid": "8cf790ad-c2a8-4c4c-8e02-6ea57062be13"
      },
      "cell_type": "markdown",
      "source": "## Submitting  Random Forests Model after use XGBoost"
    },
    {
      "metadata": {
        "_uuid": "2307c399588cd887c28413511ce9782735d3f6ce",
        "_cell_guid": "ec37fabe-1e77-4008-931c-cf4eb79798af",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom xgboost import XGBRegressor\nfrom  sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\n\niowa_train=pd.read_csv('../input/train.csv')\niowa_test=pd.read_csv('../input/test.csv')\n\ntrain=iowa_train.select_dtypes(exclude=['object'])\ntest=iowa_test.select_dtypes(exclude=['object'])\n\ny_train=train.SalePrice\nX_train= train.drop(['SalePrice'], axis=1)\n\nmy_pipeline=make_pipeline(Imputer(), XGBRegressor())\nmy_pipeline.fit(X_train,y_train)\npredictions=my_pipeline.predict(test)\nprint(predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "57b36d8ed1ee2f109cbfa69d93906a923ea63975",
        "_cell_guid": "721ec1ed-3542-4a6c-af69-188f3851ed08",
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_submission = pd.DataFrame({'ID':test.Id, 'SalePrice':predictions})\n\nmy_submission.to_csv('submission4.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0443dd302046bcd4c2f65f41459b92f318f9f69c",
        "_cell_guid": "8a6d3fed-28c5-4ab5-89c5-5eef2ee9ba2c"
      },
      "cell_type": "markdown",
      "source": "# Partial Dependence Plots\n* Pick three predictors in your project. Formulate an hypothesis about what the partial dependence plot will look like. Create the plots, and check the results against your hypothesis.\n"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "3d90fd5fdda683523bfd9b6418c1fe5e3625604b",
        "_cell_guid": "4b9f6f72-9b91-4191-a1a3-695f1bbb5825",
        "trusted": false
      },
      "cell_type": "code",
      "source": "data_predictors=['LotArea','YearBuilt','BedroomAbvGr']\nX=data[data_predictors]\ny=data.SalePrice\n\nmy_imputer=Imputer()\nX=my_imputer.fit_transform(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e4c9954ccaa7ac105410a2d6e333b1111cbfa449",
        "_cell_guid": "a070d692-8bb8-46e7-b0d4-3fbc1064f04a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\n\nmy_model=GradientBoostingRegressor()\nmy_model.fit(X,y)\n\nmy_plot=plot_partial_dependence(my_model,features=[0],X=X,\n                                feature_names=['LotArea','YearBuilt','BedroomAbvGr'],grid_resolution=15)\nmy_plot=plot_partial_dependence(my_model,features=[1], X=X,\n                                feature_names=['LotArea','YearBuilt','BedroomAbvGr'],grid_resolution=15)\nmy_plot=plot_partial_dependence(my_model,features=[2],X=X,\n                                feature_names=['LotArea','YearBuilt','BedroomAbvGr'],grid_resolution=15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c25c24f5fd3da610a45c648da6b2738962bf2e8",
        "_cell_guid": "377365c3-2aff-4144-ad09-11af2ac55515"
      },
      "cell_type": "markdown",
      "source": "# Pipelines\n* Take your modeling code and convert it to use pipelines. For now, you'll need to do one-hot encoding of categorical variables outside of the pipeline (i.e. before putting the data in the pipeline)."
    },
    {
      "metadata": {
        "_uuid": "3fc1e5664afd1caa4d9a0dcfec2fc7792cf14c1c",
        "_cell_guid": "8261fff6-aa7f-41fc-a19a-aea1f903ab9b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\n\ny=data.SalePrice\nX=data.drop(['SalePrice'], axis=1)\n\ntrain_X, test_X, train_y, test_y= train_test_split(X,y)\nen_train_X=pd.get_dummies(train_X)\nen_test_X=pd.get_dummies(test_X)\nfinal_train_X,final_test_X=en_train_X.align(en_test_X,join='inner',axis=1)\n\nmy_pipeline=make_pipeline(Imputer(),XGBRegressor(n_estimates=1000,early_stopping_rounds=5))\nmy_pipeline.fit(final_train_X , train_y)\npredictions = my_pipeline.predict(final_test_X)\nprint(mean_absolute_error(predictions,test_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b414166eca10f9b89ecef12044fc160ebfed19a8",
        "_cell_guid": "2117dfa5-a2da-4ecf-9444-663c9de49eaf"
      },
      "cell_type": "markdown",
      "source": "# Cross-Validation\n* Convert the code for your on-going project over from train-test split to cross-validation. Make sure to remove all code that divides your dataset into training and testing datasets. Leaving code you don't need any more would be sloppy.\n* Add or remove a predictor from your models. See the cross-validation score using both sets of predictors, and see how you can compare the scores."
    },
    {
      "metadata": {
        "_uuid": "6582e5002de39585c3ce3a1d87100c2ec50611ed",
        "_cell_guid": "9ef81612-4be8-4fc1-b462-4c31ec6ab7ff",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\n\ndata_predictors=['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath',\n                'BedroomAbvGr','TotRmsAbvGrd']\nX=data[data_predictors]\ny=data.SalePrice\n\nmy_pipeline=make_pipeline(Imputer(),RandomForestRegressor())\nmy_pipeline.fit(X , y)\n\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nprint(scores)\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "cdaa7403355cd16662d3ef65751ae468573e82f6",
        "_cell_guid": "33a28bae-015b-4ade-8993-fd96afae1490"
      },
      "cell_type": "markdown",
      "source": "# Final Submit"
    },
    {
      "metadata": {
        "_uuid": "f570690da7e9709aa99c924d5b3a1c5383e030a9",
        "_cell_guid": "b8da7dae-eecf-41dd-a5f1-2e245fdfc71b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom xgboost import XGBRegressor\nfrom  sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\n\niowa_train=pd.read_csv('../input/train.csv')\niowa_test=pd.read_csv('../input/test.csv')\n\ntrain=iowa_train.select_dtypes(exclude=['object'])\ntest=iowa_test.select_dtypes(exclude=['object'])\n\ny_train=train.SalePrice\nX_train= train.drop(['SalePrice'], axis=1)\n\nen_X_train=pd.get_dummies(X_train)\nen_test=pd.get_dummies(test)\nfinal_X_train,final_test=en_X_train.align(en_test,join='inner',axis=1)\n\nmy_pipeline=make_pipeline(Imputer(), XGBRegressor(n_estimates=1000,early_stopping_rounds=5))\nmy_pipeline.fit(final_X_train,y_train)\npredictions=my_pipeline.predict(final_test)\nprint(predictions)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "97a41bc01a4533648fc6d67edf6d156860d8bfda",
        "_cell_guid": "a2faab77-e92f-46ab-9cb2-99a68cf2d5e5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_submission = pd.DataFrame({'ID':final_test.Id, 'SalePrice':predictions})\n\nmy_submission.to_csv('submission5.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "774d84ae33a4e3979d75e6f018665dd964d138ec",
        "_cell_guid": "d28c31d3-b5bf-4ce2-a83c-731a873c9ddf",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}